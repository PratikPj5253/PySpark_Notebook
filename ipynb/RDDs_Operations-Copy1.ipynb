{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEzhXqKgAPpQ"
   },
   "source": [
    "# ***PySpark RDD(Resilient Distributed Datasets)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW2iB94IYadY"
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-VO1B6qYuVz"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    " # SparkContext is a central component and an entry point for creating Resilient Distributed Datasets (RDDs) and other Spark operations. It serves as the interface between your application and the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQcVwc4dY1rM"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UelHdcnUiOte"
   },
   "source": [
    "### ***Operations with Pyspark RDD***\n",
    "\n",
    "**Transformation**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow6kkQHdAp8A"
   },
   "source": [
    "**1. map()**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jK8OqnJ-Y6Q5",
    "outputId": "de2d07c6-4a2b-4bcd-fd28-ab0e954814f9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "ex_rdd = sc.parallelize([1,2,3,4,5])\n",
    "print(ex_rdd.map(lambda x: x**2).collect())\n",
    "  # The map() transformation applies a given function\n",
    "  #to each element of an RDD and returns a new RDD consisting of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAAfBqzyA6W9"
   },
   "source": [
    "**2. filter()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCZ4JSNSZcpU",
    "outputId": "1ad638c6-3f23-4126-93a6-a2819dc909f3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 14]\n"
     ]
    }
   ],
   "source": [
    "filter_rdd = sc.parallelize([11,12,13,14,15])\n",
    "print(filter_rdd.filter(lambda x: x%2 == 0).collect())\n",
    "  # The filter() transformation is used to create a new\n",
    "  #RDD by selecting elements from the original RDD that satisfy a\n",
    "  #specified condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6QxQ1NkBB0H"
   },
   "source": [
    "**3. union()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gakYbYrmekyH",
    "outputId": "16181868-6748-4571-c81f-7e67bba83582",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 42, 99, 99]\n"
     ]
    }
   ],
   "source": [
    "input = sc.parallelize([24,42,53,68,71,86,99])\n",
    "u_rdd1 = input.filter(lambda x: x%3 == 0)\n",
    "u_rdd2 = input.filter(lambda x: x%9 == 0)\n",
    "print(u_rdd1.union(u_rdd2).collect())\n",
    "#It union result of u_rdd1 and u_rdd2\n",
    "  #  The union() transformation is used to\n",
    "  #   combine two RDDs into a single RDD by stacking their elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz8UEBBkBGuc"
   },
   "source": [
    "**4. flatMap()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXP9kl5Ff9SV",
    "outputId": "4703e1fc-9d1f-4558-e8e2-024a9699ae51",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', 'Welcome', 'to', 'PySpark', 'RDD']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmap_rdd = sc.parallelize([\"Hello World\", \"Welcome to PySpark RDD\"])\n",
    "(fmap_rdd.flatMap(lambda x: x.split(\" \")).collect())\n",
    "# It split the given string separated by commas\n",
    "  # The flatMap() transformation is similar to map(),\n",
    "  #but it can return multiple output elements for each input element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgI41LxsineO"
   },
   "source": [
    "### ***Operations with Pyspark RDD***\n",
    "\n",
    "## **Actions**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0hR7jDGBbuV"
   },
   "source": [
    "**1. collect()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7kjczwTiuyn",
    "outputId": "904ffee3-6f8c-46c8-a08f-83dbfbb1c517",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 24, 33, 42, 51]\n"
     ]
    }
   ],
   "source": [
    "coll_rdd = sc.parallelize([15,24,33,42,51])\n",
    "print(coll_rdd.collect())\n",
    "  # The collect() action is used to retrieve all elements of\n",
    "  #an RDD from the Spark cluster to the driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3kZDTtzBhnV"
   },
   "source": [
    "**2. count()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjkudRBIi9o6",
    "outputId": "eba22844-b433-4f6c-a2d3-ba86abbfc314",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "cnt_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "print(cnt_rdd.count())\n",
    "  # The count() action returns the number of elements in RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkWp1DNwBl2-"
   },
   "source": [
    "**3. take()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CxoCsmcjUrH",
    "outputId": "a148ade5-3141-4751-cbd3-bc2cb48bd7d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "tk_rdd = sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "print(tk_rdd.take(4))  #means it takes first 4 elements\n",
    "  # The take(n) action retrieves the first n elements of an RDD and\n",
    "  #returns them as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwx7HDIDBp5C"
   },
   "source": [
    "**4. reduce()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fv1tliMukhON",
    "outputId": "2d342e07-f142-4487-a175-0522e237a971",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "red_rdd = sc.parallelize([1,2,3,4,5])\n",
    "print(red_rdd.reduce(lambda x, y: x+y)) # It return addition of all elements\n",
    "  # The reduce() action is used to aggregate the elements of an RDD\n",
    "  #using a specified commutative and associative binary operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub3GCdvbBtQr"
   },
   "source": [
    "**5. saveAsTextFile()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQ9eTPWgm0Me"
   },
   "outputs": [],
   "source": [
    "sv_rdd = sc.parallelize([1,2,3,4,5])\n",
    "sv_rdd.saveAsTextFile('/path/to/output/our_directory')\n",
    "  # The saveAsTextFile() action is used to write the elements of an RDD to a text file or a directory in the Hadoop Distributed File System (HDFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXDUOcBAoinT"
   },
   "source": [
    "**Operations with Pyspark Pair RDD**\n",
    "\n",
    "**Transformation**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va-uO2dgB-lL"
   },
   "source": [
    "**1. reduceByKey()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDEEIUkRn6cH",
    "outputId": "d429164a-0a55-4449-d9b9-036fad276df1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Amilia', 81), ('Marie', 69), ('Gomez', 48), ('Jesse', 61), ('Frank', 94)]\n"
     ]
    }
   ],
   "source": [
    "mks_rdd = sc.parallelize([('Jesse', 38), ('Frank', 49), ('Amilia', 43), ('Marie', 33), ('Gomez', 48), ('Jesse', 23), ('Frank', 45), ('Amilia', 38), ('Marie', 36)])\n",
    "print(mks_rdd.reduceByKey(lambda x, y: x + y).collect())\n",
    " # It print unique key and then add that key values\n",
    " #The reduceByKey() transformation is used on Pair RDDs and performs a reduction\n",
    " #on the values of each key using the specified reduce function.\n",
    " # For ex: ('Amilia', 43), ('Amilia', 38)\n",
    "      # then it print ('Amilia', 43+38)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TITL2rpCDAV"
   },
   "source": [
    "**2. groupByKey()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VU82nlweoRAn",
    "outputId": "f04662dc-e6cc-49a4-c433-5229275e4674",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amilia [43, 38]\n",
      "Marie [33, 36]\n",
      "Gomez [48]\n",
      "Jesse [38, 23]\n",
      "Frank [49, 45]\n"
     ]
    }
   ],
   "source": [
    "mks_rdd = sc.parallelize([('Jesse', 38), ('Frank', 49), ('Amilia', 43), ('Marie', 33), ('Gomez', 48), ('Jesse', 23), ('Frank', 45), ('Amilia', 38), ('Marie', 36)])\n",
    "dct_rdd = mks_rdd.groupByKey().collect()\n",
    "\n",
    "for key, value in dct_rdd:\n",
    "  print(key, list(value))\n",
    "\n",
    "  # The groupByKey() transformation groups the values of each key\n",
    "  #in the Pair RDD into an iterable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSMA88DkCGnw"
   },
   "source": [
    "**3. sortByKey()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGur9W6JyXfc",
    "outputId": "f1aab6a5-97a1-4e67-8460-041255fd4e55",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 5), (1, 10), (2, 4), (2, 6), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "srt_rdd = sc.parallelize([(1, 5),(1, 10),(2, 4),(3, 1),(2, 6)])\n",
    "print(srt_rdd.sortByKey().collect())\n",
    "# The sortByKey() can be used to sort the pair RDD based on keys.\n",
    "  # The sortByKey() transformation is used to sort the elements of\n",
    "  # a Pair RDD by their keys in ascending or descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmNnkr6yyMoz"
   },
   "source": [
    "**Operations with Pyspark Pair RDD**\n",
    "\n",
    "**Action**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybF-dVw5CPLy"
   },
   "source": [
    "**1. countByKey()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg__v3rzyUEG",
    "outputId": "678f3bba-e1e5-4dd4-fca1-ba66347a159b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesse 2\n",
      "Frank 2\n",
      "Amilia 2\n",
      "Marie 2\n",
      "Gomez 1\n"
     ]
    }
   ],
   "source": [
    "mks_rdd = sc.parallelize([('Jesse', 38), ('Frank', 49), ('Amilia', 43), ('Marie', 33), ('Gomez', 48), ('Jesse', 23), ('Frank', 45), ('Amilia', 38), ('Marie', 36)])\n",
    "dct_rdd = mks_rdd.countByKey().items()\n",
    "\n",
    "for key, value in dct_rdd:\n",
    "  print(key, value)\n",
    "  # The countByKey() action is used on a Pair RDD to count the\n",
    "  #number of occurrences of each unique key."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
