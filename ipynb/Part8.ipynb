{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd9f025-61d6-4ba8-bf85-b56ee0cb928d",
   "metadata": {},
   "source": [
    "## Manually Creating Dataframe By Adding our own data in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee0a0ecb-9123-41b6-9f3c-34cbc24c3b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\PRATJADH\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0bc5796-fba7-4565-9887-234d0c2e1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0acf0a0e-7dcf-49a0-8e98-2c9e1feeed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d17a3b3a-ab19-45f9-a27e-425b4d1cd334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3336f7ad-ed29-4353-916d-4a15d4fcce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DataFrame1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "535272a5-94ea-4a46-8ec6-562f7dd64836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LIN51013396.corp.capgemini.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2427534a890>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a35aaeb-7796-4820-a09a-0478a0f1b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"Pratik\"), (2, \"Harshad\"), (3, \"Shubham\"), (4, \"Saurabh\"), (5, \"Omkar\"), (6, \"Chinmay\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98b1fe02-fed1-4829-9689-de6951ebb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae391d2f-2aaf-4899-8bbc-4492601b3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bca0c63-f068-4a28-98da-9508b2b88132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|   Name|\n",
      "+---+-------+\n",
      "|  1| Pratik|\n",
      "|  2|Harshad|\n",
      "|  3|Shubham|\n",
      "|  4|Saurabh|\n",
      "|  5|  Omkar|\n",
      "|  6|Chinmay|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3feadb53-7b16-4627-a0da-2a53e78b2c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|   Name|\n",
      "+---+-------+\n",
      "|  1| Pratik|\n",
      "|  2|Harshad|\n",
      "|  3|Shubham|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9832e76-7ca1-49de-9819-0c7399ddcf90",
   "metadata": {},
   "source": [
    "## Convert RDD to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbf3560f-bcd7-4296-a6d1-ea45c939ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "79802c6b-17ab-4bc2-bfa9-e0b0430ad5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23fda140-b568-4770-bf82-17e7ab729ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, \"Pratik\"), (2, \"Harshad\"), (3, \"Shubham\"), (4, \"Saurabh\"), (5, \"Omkar\"), (6, \"Chinmay\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60fbb256-e9fe-40c2-ab93-a8707b724f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[113] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cda02d1b-c339-45ae-afa2-ce8320762a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_To_df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4349db2a-8dc8-4cea-bf8d-1bd347669031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_To_df\n",
    " # Now it convert RDD into dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53305785-7997-4b1e-b5fb-a8d8e62882dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1=1, _2='Pratik'), Row(_1=2, _2='Harshad'), Row(_1=3, _2='Shubham'), Row(_1=4, _2='Saurabh'), Row(_1=5, _2='Omkar'), Row(_1=6, _2='Chinmay')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_To_df.rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6d0bf-00e9-4500-b75e-ba9f388fabc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f63e618c-b868-4f7a-8948-ee2cc9af28df",
   "metadata": {},
   "source": [
    "## Read text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ce44f74-d967-4e66-b535-8e6a02de1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.text('df_txt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2218bea9-f54a-41df-b9bf-cc13b6f7b789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|11-Dec-23\\tIntrod...|\n",
      "|12-Dec-23\\tIntrod...|\n",
      "|13-Dec-23\\tCore P...|\n",
      "|14-Dec-23\\tPracti...|\n",
      "|15-Dec-23\\tPracti...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e4c03-bfe6-43a2-b066-276e586d6f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c14bbc-1c35-43ef-bac7-c903f42e7ef0",
   "metadata": {},
   "source": [
    "## Read Paraquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61c56977-0fb7-4592-b9bf-92869f1dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.parquet('C:/Users/PRATJADH/Downloads/mt cars.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01a5643a-3aa5-4771-bf83-28e10beab4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              model| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n",
      "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
      "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
      "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
      "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n",
      "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n",
      "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n",
      "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n",
      "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n",
      "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
      "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
      "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
      "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
      "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
      "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n",
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b338430-5506-4059-9764-653553730a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              model|\n",
      "+-------------------+\n",
      "|          Mazda RX4|\n",
      "|      Mazda RX4 Wag|\n",
      "|         Datsun 710|\n",
      "|     Hornet 4 Drive|\n",
      "|  Hornet Sportabout|\n",
      "|            Valiant|\n",
      "|         Duster 360|\n",
      "|          Merc 240D|\n",
      "|           Merc 230|\n",
      "|           Merc 280|\n",
      "|          Merc 280C|\n",
      "|         Merc 450SE|\n",
      "|         Merc 450SL|\n",
      "|        Merc 450SLC|\n",
      "| Cadillac Fleetwood|\n",
      "|Lincoln Continental|\n",
      "|  Chrysler Imperial|\n",
      "|           Fiat 128|\n",
      "|        Honda Civic|\n",
      "|     Toyota Corolla|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select('model').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be83d4c-5d35-4076-a10a-c950c4b9cd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab2759-2139-4063-955d-53c11aa6c584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57b9ce54-5985-451e-840b-55dc224bd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.json('C:/Users/PRATJADH/Downloads/dwsample1-json.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1ba857d-466f-474b-a968-5f63a709a3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62ec34b3-6ca9-4ecd-b53d-c772085ff541",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m corrupt_records_count \u001b[38;5;241m=\u001b[39m \u001b[43mdf4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf4\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_corrupt_record\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNotNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "corrupt_records_count = df4.filter(df4[\"_corrupt_record\"].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30bedce-4187-4709-9993-b204915d017d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd50d3-982a-404d-8a28-45f882c0ca96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899a450-79dc-4d67-912d-a557d721c8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
